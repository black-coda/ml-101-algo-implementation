{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pathlib import Path, PosixPath\n",
    "import tarfile\n",
    "import urllib\n",
    "import re\n",
    "import math as mt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urlextract import URLExtract\n",
    "from email import policy, message\n",
    "from email.parser import BytesParser\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, precision_score, recall_score, f1_score)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"classification\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "url_extractor = URLExtract()\n",
    "\n",
    "\n",
    "#! STEP 1: Fetch the data\n",
    "\n",
    "\n",
    "def fetch_spam_data():\n",
    "    spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"\n",
    "    spam_url = spam_root + \"20030228_spam.tar.bz2\"\n",
    "\n",
    "    spam_path = Path() / \"datasets\" / \"spam\"\n",
    "    spam_path.mkdir(parents=True, exist_ok=True)\n",
    "    for dir_name, tar_name, url in (\n",
    "        (\"easy_ham\", \"ham\", ham_url),\n",
    "        (\"spam\", \"spam\", spam_url),\n",
    "    ):\n",
    "        if not (spam_path / dir_name).is_dir():\n",
    "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")\n",
    "            print(\"Downloading\", path)\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            tar_bz2_file = tarfile.open(path)\n",
    "            tar_bz2_file.extractall(path=spam_path)\n",
    "            tar_bz2_file.close()\n",
    "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n",
    "\n",
    "\n",
    "# * Fetch the data and list the filenames\n",
    "ham_dir, spam_dir = fetch_spam_data()\n",
    "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]\n",
    "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]\n",
    "\n",
    "common_divisor = mt.gcd(len(ham_filenames), len(spam_filenames))\n",
    "ham_sample_size = len(ham_filenames) // common_divisor\n",
    "spam_sample_size = len(spam_filenames) // common_divisor\n",
    "\n",
    "\n",
    "print(f\"Number of ham emails: {len(ham_filenames)}\")\n",
    "print(f\"Number of spam emails: {len(spam_filenames)}\")\n",
    "print(\"Total number of emails:\", len(ham_filenames) + len(spam_filenames))\n",
    "print(f\"The ratio of ham to spam emails is {ham_sample_size}:{spam_sample_size}\")\n",
    "\n",
    "\n",
    "#! STEP 2: Parse the emails\n",
    "EmailType = Literal[\"ham\", \"spam\"]\n",
    "\n",
    "\n",
    "def load_email(filename: Path):\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return BytesParser(policy=policy.default).parse(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load email {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def display_email_info(email) -> None:\n",
    "    print(type(email))\n",
    "    is_multipart = email.is_multipart()\n",
    "    content_type = email.get_content_type()\n",
    "    print(f\"Subject: {email.get('subject')}\")\n",
    "    print(f\"From: {email.get('from')}\")\n",
    "    print(f\"Is multipart: {is_multipart}\")\n",
    "    print(f\"Content type: {content_type}\")\n",
    "\n",
    "\n",
    "print(\"=== HAM EMAIL ===\")\n",
    "ham_email = load_email(ham_filenames[0])\n",
    "display_email_info(ham_email)\n",
    "\n",
    "print(\"\\n=== SPAM EMAIL ===\")\n",
    "spam_email = load_email(spam_filenames[0])\n",
    "display_email_info(spam_email)\n",
    "\n",
    "\n",
    "#! STEP 3: Preprocess the emails\n",
    "\n",
    "\n",
    "def parse_html_to_text(html) -> str:\n",
    "    \"\"\"Convert HTML to clean plain text\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    for element in soup([\"script\", \"style\"]):\n",
    "        element.decompose()\n",
    "\n",
    "    # Get text with separator between elements\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "def email_to_text(email: message.EmailMessage) -> str:\n",
    "    \"\"\"Convert an email to plain text.\"\"\"\n",
    "    text_parts = []  # Better name\n",
    "\n",
    "    # Add subject\n",
    "    email_subject = email.get(\"subject\", \"\")\n",
    "    if email_subject:\n",
    "        text_parts.append(email_subject)\n",
    "\n",
    "    if not email.is_multipart():\n",
    "        content_type = email.get_content_type()\n",
    "        try:\n",
    "            content = email.get_content()\n",
    "            if content_type == \"text/plain\":\n",
    "                text_parts.append(content)\n",
    "            elif content_type == \"text/html\":\n",
    "                text_parts.append(parse_html_to_text(content))\n",
    "        except:\n",
    "            content = str(email.get_payload())\n",
    "            text_parts.append(content)\n",
    "    else:\n",
    "        for part in email.walk():\n",
    "            part_content_type = part.get_content_type()\n",
    "            try:\n",
    "                if part_content_type == \"text/plain\":\n",
    "                    text_parts.append(part.get_content())\n",
    "                elif part_content_type == \"text/html\":\n",
    "                    text_parts.append(parse_html_to_text(part.get_content()))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return \" \".join(text_parts)\n",
    "\n",
    "\n",
    "#! STEP 4: Create Dataset\n",
    "\n",
    "\n",
    "def load_all_emails(filenames: List[Path]):\n",
    "    emails = []\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            email = load_email(filename)\n",
    "            emails.append(email)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading email {filename}: {e}\")\n",
    "            continue\n",
    "    return emails\n",
    "\n",
    "\n",
    "ham_emails = load_all_emails(ham_filenames)\n",
    "spam_emails = load_all_emails(spam_filenames)\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    ham_emails: List[message.EmailMessage], spam_emails: List[message.EmailMessage]\n",
    "):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for email in ham_emails:\n",
    "        text = email_to_text(email)\n",
    "        texts.append(text)\n",
    "        labels.append(0)  # Ham label\n",
    "\n",
    "    for email in spam_emails:\n",
    "        text = email_to_text(email)\n",
    "        texts.append(text)\n",
    "        labels.append(1)  # Spam label\n",
    "\n",
    "    return pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "\n",
    "def create_dataset_np(\n",
    "    ham_emails: List[message.EmailMessage], spam_emails: List[message.EmailMessage]\n",
    "):\n",
    "    X = np.array(\n",
    "        [email_to_text(email) for email in ham_emails + spam_emails], dtype=object\n",
    "    )\n",
    "    y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = create_dataset_np(ham_emails, spam_emails)\n",
    "\n",
    "print(f\"Loaded {len(ham_emails)} ham emails\")\n",
    "print(f\"Loaded {len(spam_emails)} spam emails\")\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFirst 5 labels: {y[:5]}\")\n",
    "print(f\"Last 5 labels: {y[-5:]}\")\n",
    "print(f\"\\nSample text (first 150 chars):\\n{X[0][:150]}\")\n",
    "\n",
    "\n",
    "#! STEP 5: Split the dataset\n",
    "# * stratify=y: This is the most important part for classification. It ensures the proportion of classes is the same in both sets. If your data is 90% \"ham\" and 10% \"spam,\" stratify makes sure both your training and testing sets are also 90/10. Without it, you might accidentally end up with all the \"spam\" in your test set and none in your training set!\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} emails\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} emails\")\n",
    "\n",
    "print(f\"Training set: {np.bincount(y_train)} (ham, spam)\")\n",
    "print(f\"Testing set: {np.bincount(y_test)} (ham, spam)\")\n",
    "\n",
    "print(f\"\\nFirst 5 training labels: {y_train[:5]}\")\n",
    "print(f\"First 5 testing labels: {y_test[:5]}\")\n",
    "\n",
    "#! STEP 6: Text preprocessing and stemming\n",
    "\n",
    "\n",
    "def urlExtractor(text):\n",
    "    return url_extractor.find_urls(text)\n",
    "\n",
    "\n",
    "def convert_urls(text: str) -> str:\n",
    "    urls = urlExtractor(text)\n",
    "    for url in urls:\n",
    "        text = re.sub(re.escape(url), \" URL \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    text: str,\n",
    "    lower_case=True,\n",
    "    replace_urls=True,\n",
    "    replace_emails=True,\n",
    "    replace_numbers=True,\n",
    "    remove_punctuation=True,\n",
    "    stemming=True,\n",
    "):\n",
    "    # 1. Lowercase\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 2. Replace URLs\n",
    "    if replace_urls:\n",
    "        text = convert_urls(text)\n",
    "\n",
    "    # 3. Replace emails\n",
    "    if replace_emails:\n",
    "        email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\"\n",
    "        text = re.sub(email_pattern, \" EMAIL \", text)\n",
    "\n",
    "    # 4. Replace numbers\n",
    "    if replace_numbers:\n",
    "        number_pattern = r\"\\b\\d+(\\.\\d+)?\\b\"\n",
    "        text = re.sub(number_pattern, \" NUMBER \", text)\n",
    "\n",
    "    # 5. Remove unwanted punctuation\n",
    "    if remove_punctuation:\n",
    "        # keep ! $ % ' -\n",
    "        text = re.sub(r\"[^\\w\\s!\\$%'\\-]\", \" \", text)\n",
    "\n",
    "    # 6. Stemming\n",
    "    if stemming:\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        text = \" \".join(words)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "class TextPreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lower_case=True,\n",
    "        replace_urls=True,\n",
    "        replace_emails=True,\n",
    "        replace_numbers=True,\n",
    "        remove_punctuation=True,\n",
    "        stemming=True,\n",
    "    ):\n",
    "        self.lower_case = lower_case\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_emails = replace_emails\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.stemming = stemming\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No training needed\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            cleaned = preprocess_text(\n",
    "                text,\n",
    "                lower_case=self.lower_case,\n",
    "                replace_urls=self.replace_urls,\n",
    "                replace_emails=self.replace_emails,\n",
    "                replace_numbers=self.replace_numbers,\n",
    "                remove_punctuation=self.remove_punctuation,\n",
    "                stemming=self.stemming,\n",
    "            )\n",
    "            X_transformed.append(cleaned)\n",
    "\n",
    "        return np.array(X_transformed)\n",
    "\n",
    "\n",
    "#! STEP 7: Vectorization\n",
    "# TF → Term Frequency\n",
    "# How often a word appears in a document.\n",
    "\n",
    "# IDF → Inverse Document Frequency\n",
    "# How rare that word is across all documents.\n",
    "\n",
    "tfidi_vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,  # Already done in preprocessingp\n",
    "    ngram_range=(1, 2),  # unigrams and bigrams\n",
    "    min_df=3,  # Ignore very rare words\n",
    "    max_df=0.9,  # Ignore very common words\n",
    "    max_features=10000,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True,\n",
    "    binary=False,\n",
    ")\n",
    "\n",
    "#! Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", TextPreprocessingTransformer()),\n",
    "    (\"vectorizer\", tfidi_vectorizer),\n",
    "    (\"classifier\", MultinomialNB(alpha=0.1)), \n",
    "])\n",
    "\n",
    "# ! STEP 8: Train the model and evaluate\n",
    "print(\"=== Classification Report ===\")\n",
    "# Train the entire pipeline\n",
    "print(\"Training pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.2%}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.2%}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(cm)\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"True Negatives (Ham→Ham): {cm[0,0]}\")\n",
    "print(f\"False Positives (Ham→Spam): {cm[0,1]} ← CRITICAL!\")\n",
    "print(f\"False Negatives (Spam→Ham): {cm[1,0]} ← Spam that got through\")\n",
    "print(f\"True Positives (Spam→Spam): {cm[1,1]}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], \n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(IMAGES_PATH / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = pipeline.predict_proba(X_test)\n",
    "print(\"\\n=== Prediction Probability Stats ===\")\n",
    "print(f\"Spam probabilities - Min: {y_pred_proba[:, 1].min():.4f}, Max: {y_pred_proba[:, 1].max():.4f}\")\n",
    "print(f\"Mean spam probability for actual spam: {y_pred_proba[y_test==1, 1].mean():.4f}\")\n",
    "print(f\"Mean spam probability for actual ham: {y_pred_proba[y_test==0, 1].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
